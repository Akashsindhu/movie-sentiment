{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter 15 Embedding + CNN Model for Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akashsindhu/movie-sentiment/blob/master/chapter_15_Embedding_%2B_CNN_Model_for_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-T8xQyZlfs6",
        "colab_type": "text"
      },
      "source": [
        "1. Movie Review Dataset\n",
        "2. Data Preparation\n",
        "3. Train CNN With Embedding Layer\n",
        "4. Evaluate Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4VnAOJdmRCH",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "b6a133be-9d07-4ef5-e2ec-3e16e22d5277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#@title Default title text\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBIwhwfdfcbj",
        "colab_type": "code",
        "outputId": "8192dd99-0dd1-46c8-feed-07134cfd5b37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /gdrive/My Drive/Colab Notebooks/NLTK/review_polarity"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/Colab Notebooks/NLTK/review_polarity\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plk4MVYwpBPM",
        "colab_type": "text"
      },
      "source": [
        "### Loading and Cleaning Reviews\n",
        "The text data is already pretty clean; not much preparation is required. Without getting bogged down too much in the details, we will prepare the data using the following way:\n",
        "\n",
        "1.   Split tokens on white space.\n",
        "2.   Remove all punctuation from words.\n",
        "3.   Remove all words that are not purely comprised of alphabetical characters.\n",
        "4. Remove all words that are known stop words.  Remove all words that have a length ≤ 1 character\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnV8EAaEnVQB",
        "colab_type": "code",
        "outputId": "84f3ed2a-99f7-42c1-f218-1c4cda748455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7gdRNKypmFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the doc into memory \n",
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# clean the text from the file and convert to tokens\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space \n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering \n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    #remove punctuations from each word \n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining words that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stopwords \n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens \n",
        "    tokens = [w for w in tokens if len(w) > 1]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# load the document \n",
        "# filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
        "# text = load_doc(filename)\n",
        "# tokens = clean_doc(text)\n",
        "# print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi56MjeHsZzS",
        "colab_type": "text"
      },
      "source": [
        "### Define a vocabulary\n",
        "It is important to deﬁne a vocabulary of known words when using a text model. The more words, the larger the representation of documents, therefore it is important to constrain the words to only those believed to be predictive. This is diﬃcult to know beforehand and often it is important to test diﬀerent hypotheses about how to construct a useful vocabulary. We have already seen how we can remove punctuation and numbers from the vocabulary in the previous section. We can repeat this for all documents and build a set of all known words. \n",
        "\n",
        "We can develop a vocabulary as a Counter, which is a dictionary mapping of words and their count that allows us to easily update and query. Each document can be added to the counter (a new function called add doc to vocab()) and we can step over all of the reviews in the negative directory and then the positive directory (a new function called process docs()). The complete example is listed below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMO29287sAUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re \n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhbPtJbMIxiT",
        "colab_type": "code",
        "outputId": "622b1ece-5ccd-4610-bde4-0c098462843d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "#load doc from memory is already defined above\n",
        "# clean the doc already defined above \n",
        "\n",
        "# load doc and add to vocab \n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "    text = load_doc(filename)\n",
        "    tokens = clean_doc(text)\n",
        "    vocab.update(tokens)\n",
        "\n",
        "# step over all the docs in negative and positive \n",
        "def step_alldoc(directory, vocab):\n",
        "    for filename in listdir(directory):\n",
        "        if filename.startswith('cv9'):\n",
        "            next\n",
        "        path = directory + '/' + filename\n",
        "        add_doc_to_vocab(path, vocab)\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "step_alldoc('txt_sentoken/pos/', vocab)\n",
        "step_alldoc('txt_sentoken/neg/', vocab)\n",
        "\n",
        "print(len(vocab))\n",
        "print(vocab.most_common(50))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46557\n",
            "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjBmpx9XKbrN",
        "colab_type": "code",
        "outputId": "604add6c-c906-48df-8a66-e772c3acde49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "occurance = 2\n",
        "tokens = [k for k, c in vocab.items() if c >= occurance]\n",
        "print(len(tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du4_APP4haf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save tokens to a file \n",
        "def save_file(vocab):\n",
        "    filename = 'vocab.txt'\n",
        "    # join the tokens to a line \n",
        "    data = '\\n'.join(vocab)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "save_file(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSwC_FEL4Iac",
        "colab_type": "text"
      },
      "source": [
        "Train CNN with Embedding layer \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTxmTb50C-i2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQOewlVq5Hx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_doc_per_string(doc, vocab):\n",
        "    tokens = clean_doc(doc)\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "def updated_process_doc(directory, vocab, is_train):\n",
        "    documents = list()\n",
        "    for filename in listdir(directory):\n",
        "        if is_train and filename.startswith('cv9'):\n",
        "            next\n",
        "        if not is_train and not filename.startswith('cv9'):\n",
        "            next\n",
        "        path = directory + '/' + filename\n",
        "        text = load_doc(path)\n",
        "        tokens = one_doc_per_string(text, vocab)\n",
        "        documents.append(tokens)\n",
        "    return documents\n",
        "\n",
        "def load_clean_docs(vocab, is_train):\n",
        "    neg = updated_process_doc('txt_sentoken/neg/', vocab, is_train)\n",
        "    pos = updated_process_doc('txt_sentoken/pos/', vocab, is_train)\n",
        "    doc = neg + pos \n",
        "    # prepare labels \n",
        "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "    return doc, labels \n",
        "\n",
        "# fit the tokenizer \n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "    #integer encode \n",
        "    encoded = tokenizer.texts_to_sequences(docs)\n",
        "    #pad sequence\n",
        "    padded = pad_sequences(encoded, maxlen = max_length, padding = 'post')\n",
        "    return padded\n",
        "\n",
        "def model(vocab_size, max_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer= 'adam', metrics = ['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmsiNEtfi2X0",
        "colab_type": "code",
        "outputId": "2534086b-e43f-410e-c213-30731a389d6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        }
      },
      "source": [
        "# load the vocabulary \n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load training data\n",
        "train_docs, y_train = load_clean_docs(vocab, True)\n",
        "#create tokenizer \n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "\n",
        "#define vocab size \n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print('vocab size: %d' % vocab_size)\n",
        "#calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print(\"max length: %d\" % max_length)\n",
        "\n",
        "#encode data \n",
        "X_train = encode_docs(tokenizer, max_length, train_docs)\n",
        "#define model\n",
        "model = model(vocab_size, max_length)\n",
        "\n",
        "#fit the model \n",
        "model.fit(X_train, y_train, epochs= 10, verbose = 1)\n",
        "# save the model \n",
        "model.save('model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size: 27140\n",
            "max length: 1319\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 1319, 100)         2714000   \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 1312, 32)          25632     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 656, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 20992)             0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                209930    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 2,949,573\n",
            "Trainable params: 2,949,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "2000/2000 [==============================] - 2s 1ms/step - loss: 0.6957 - acc: 0.5240\n",
            "Epoch 2/10\n",
            "2000/2000 [==============================] - 1s 544us/step - loss: 0.5706 - acc: 0.6730\n",
            "Epoch 3/10\n",
            "2000/2000 [==============================] - 1s 550us/step - loss: 0.1821 - acc: 0.9340\n",
            "Epoch 4/10\n",
            "2000/2000 [==============================] - 1s 534us/step - loss: 0.0157 - acc: 0.9985\n",
            "Epoch 5/10\n",
            "2000/2000 [==============================] - 1s 533us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "2000/2000 [==============================] - 1s 554us/step - loss: 8.6603e-04 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "2000/2000 [==============================] - 1s 550us/step - loss: 5.3457e-04 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "2000/2000 [==============================] - 1s 540us/step - loss: 3.6834e-04 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "2000/2000 [==============================] - 1s 532us/step - loss: 2.7004e-04 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "2000/2000 [==============================] - 1s 537us/step - loss: 2.0784e-04 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29siFadTKZE9",
        "colab_type": "text"
      },
      "source": [
        "evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PoSeF5WNEd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
        "    line = one_doc_per_string(review, vocab)\n",
        "    padded = encode_docs(tokenizer, max_length, [line])\n",
        "    # predict sentiment \n",
        "    predicted = model.predict(padded, verbose = 0)\n",
        "    # retrive predicted percentage and label\n",
        "    percent_pos = predicted[0,0]\n",
        "    if round(percent_pos) == 0:\n",
        "        return (1-percent_pos), 'NEGATIVE'\n",
        "    return percent_pos, 'POSITIVE'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plu17thEFhRA",
        "colab_type": "code",
        "outputId": "c057ccbe-1e22-4d37-b3ac-07a76d2098a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "#load all reviews \n",
        "train_docs, y_train  = load_clean_docs(vocab, True)\n",
        "test_docs, y_test = load_clean_docs(vocab, False)\n",
        "#createthe tokenizer \n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# define the vocab size \n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "\n",
        "# calculate the maximum seuence length \n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('Maximum length: %d' % max_length)\n",
        "\n",
        "# encoded data \n",
        "X_train = encode_docs(tokenizer, max_length, train_docs)\n",
        "X_test = encode_docs(tokenizer, max_length, test_docs)\n",
        "\n",
        "# load the model \n",
        "model = load_model('model.h5')\n",
        "# evaluate model on training dataset \n",
        "_, acc = model.evaluate(X_train, y_test, verbose = 1)\n",
        "print( 'Train Accuracy: %f' % (acc*100))\n",
        "\n",
        "_, acc = model.evaluate(X_test, y_test, verbose = 1)\n",
        "print ('Test Accuracy: %f' % (acc*100))\n",
        "\n",
        "#test positive text\n",
        "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print(\"Review: [%s]\\nSentiment: %s (%.3f%%)\" % (text, sentiment, percent*100))\n",
        "\n",
        "# test negative text \n",
        "text = 'This is a bad movie. Do not watch it. It sucks'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print(\"Review: [%s]\\nSentiment: %s (%.3f%%)\" % (text, sentiment, percent*100))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 27140\n",
            "Maximum length: 1319\n",
            "2000/2000 [==============================] - 1s 366us/step\n",
            "Train Accuracy: 100.000000\n",
            "2000/2000 [==============================] - 0s 189us/step\n",
            "Test Accuracy: 100.000000\n",
            "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
            "Sentiment: NEGATIVE (99.984%)\n",
            "Review: [This is a bad movie. Do not watch it. It sucks]\n",
            "Sentiment: NEGATIVE (99.985%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpwEzX_RbXBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}