{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "movie_sentiment_bag_of_words.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akashsindhu/movie-sentiment-using-bag-of-words/blob/master/movie_sentiment_bag_of_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NolHehZ5S7Z",
        "colab_type": "code",
        "outputId": "539d365a-9a60-430f-e756-cfbb680a62c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-Bni0Cp5O3l",
        "colab_type": "code",
        "outputId": "afb156ca-a786-4f40-d587-fc2616317da9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd My\\ Drive/Colab\\ Notebooks/NLTK/review_polarity"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/Colab Notebooks/NLTK/review_polarity\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obT0B01K5Y_q",
        "colab_type": "code",
        "outputId": "478c8ce7-e5d9-41a7-a52a-918c0bb28f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# now collecting all three functions together \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def load_doc(filename):\n",
        "  #open the file as read only\n",
        "\n",
        "  file = open(filename, 'r')\n",
        "  # read the all the text \n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "def clean_doc(file):\n",
        "  tokens = file.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "  # remove punctuation from each word \n",
        "  tokens = [re_punc.sub('',w) for w in tokens]\n",
        "\n",
        "  # remove tokens that are not alphabets \n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "  # remove english stop words using NLTK\n",
        "  stopword = stopwords.words('english')\n",
        "  tokens = [w for w in tokens if not w in stopword]\n",
        "\n",
        "  # remove short tokens \n",
        "  tokens = [word for word in tokens if (len(word) > 1)]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "  #load the doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean the doc \n",
        "  tokens = clean_doc(doc)\n",
        "  # update the vocab\n",
        "  vocab.update(tokens)\n",
        "\n",
        "def process_doc(directory, vocab):\n",
        "  count = 0\n",
        "  for file in listdir(directory):\n",
        "    if not file.endswith('.txt'):\n",
        "      next\n",
        "    count  = count + 1\n",
        "    path = directory + '/' + file\n",
        "    add_doc_to_vocab(path, vocab)\n",
        "    # return vocab\n",
        "  print(count)\n",
        "\n",
        "# initialize vocab \n",
        "vocab = Counter()\n",
        "# pass all documents to process_doc function \n",
        "process_doc('txt_sentoken/neg/', vocab)\n",
        "process_doc('txt_sentoken/pos/', vocab)\n",
        "print(len(vocab))\n",
        "\n",
        "# # load doc into memory \n",
        "# def load_doc(filename): \n",
        "#   # open the file as read only \n",
        "#   file = open(filename, 'r') \n",
        "#   # read all text \n",
        "#   text = file.read() \n",
        "#   # close the file \n",
        "#   file.close() \n",
        "#   return text\n",
        "\n",
        "# # turn a doc into clean tokens \n",
        "# def clean_doc(doc): \n",
        "#   # split into tokens by white space \n",
        "#   tokens = doc.split() \n",
        "#   # prepare regex for char filtering \n",
        "#   re_punc = re.compile('[%s]' % re.escape(string.punctuation)) \n",
        "#   # remove punctuation from each word \n",
        "#   tokens = [re_punc.sub('', w) for w in tokens] \n",
        "#   # remove remaining tokens that are not alphabetic \n",
        "#   tokens = [word for word in tokens if word.isalpha()] \n",
        "#   # filter out stop words \n",
        "#   stop_words = set(stopwords.words('english')) \n",
        "#   tokens = [w for w in tokens if not w in stop_words] \n",
        "#   # filter out short tokens \n",
        "#   tokens = [word for word in tokens if len(word) > 1] \n",
        "#   return tokens\n",
        "\n",
        "# # load doc and add to vocab \n",
        "# def add_doc_to_vocab(filename, vocab): \n",
        "#   # load doc \n",
        "#   doc = load_doc(filename) \n",
        "#   # clean doc \n",
        "#   tokens = clean_doc(doc) \n",
        "#   # update counts \n",
        "#   vocab.update(tokens)\n",
        "\n",
        "# def process_docs(directory, vocab): \n",
        "#   # walk through all files in the folder \n",
        "#   for filename in listdir(directory): \n",
        "#     # skip files that do not have the right extension \n",
        "#     if not filename.endswith(\".txt\"): \n",
        "#       next \n",
        "#     # create the full path of the file to open \n",
        "#     path = directory + '/' + filename \n",
        "#     # add doc to vocab \n",
        "#     add_doc_to_vocab(path, vocab)\n",
        "\n",
        "# # define vocab \n",
        "# vocab = Counter() \n",
        "# # add all docs to vocab \n",
        "# process_docs('txt_sentoken/neg', vocab) \n",
        "# process_docs('txt_sentoken/pos', vocab) \n",
        "# # print the size of the vocab \n",
        "# print(len(vocab))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "1000\n",
            "1000\n",
            "46557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Vb1ZX7kSt50",
        "colab_type": "code",
        "outputId": "0a4f4d57-e6c9-4235-b905-80e289930b72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "min_occurance = 2\n",
        "vocab = [w for w,c in vocab.items() if c >= min_occurance]\n",
        "len(vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27139"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At9_sfyLUi7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save list to file for later use. \n",
        "def save_list(lines, filename):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "save_list(vocab, 'vocab.txt')\n",
        "\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvTlBqClWQZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doc_to_line(filename, vocab):\n",
        "  # load the document \n",
        "  doc = load_doc(filename)\n",
        "  # clean the document \n",
        "  tokens = clean_doc(doc)\n",
        "  # filter by vocab \n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  # in line \n",
        "  return ' '.join(tokens)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7uzEDvKb6JB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now step through all the files in neg and pos folder and covert each file to line of tokens \n",
        "\n",
        "def process_doc(directory, vocab):\n",
        "  lines = list()\n",
        "  for file in listdir(directory):\n",
        "    if not file.endswith('txt'):\n",
        "      next\n",
        "    path = directory + '/' + file\n",
        "    line = doc_to_line(path, vocab)\n",
        "    lines = append.line()\n",
        "  return lines\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtWqapGTeH9J",
        "colab_type": "code",
        "outputId": "b538e562-fea8-47a2-df54-eb34d2069dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# now collecting all three functions together \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def load_doc(filename):\n",
        "  #open the file as read only\n",
        "\n",
        "  file = open(filename, 'r')\n",
        "  # read the all the text \n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "def clean_doc(file):\n",
        "  tokens = file.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "  # remove punctuation from each word \n",
        "  tokens = [re_punc.sub('',w) for w in tokens]\n",
        "\n",
        "  # remove tokens that are not alphabets \n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "  # remove english stop words using NLTK\n",
        "  stopword = stopwords.words('english')\n",
        "  tokens = [w for w in tokens if not w in stopword]\n",
        "\n",
        "  # remove short tokens \n",
        "  tokens = [word for word in tokens if (len(word) > 1)]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "def doc_to_line(filename, vocab):\n",
        "  # load the document \n",
        "  doc = load_doc(filename)\n",
        "  # clean the document \n",
        "  tokens = clean_doc(doc)\n",
        "  # filter by vocab \n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  # in line \n",
        "  return ' '.join(tokens)\n",
        "\n",
        "def process_doc(directory, vocab):\n",
        "  lines = list()\n",
        "  for file in listdir(directory):\n",
        "    if not file.endswith('txt'):\n",
        "      next\n",
        "    path = directory + '/' + file\n",
        "    line = doc_to_line(path, vocab)\n",
        "    lines.append(line)\n",
        "  return lines\n",
        "\n",
        "def save_list(lines, filename):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "\n",
        "def load_clean_dataset(vocab):\n",
        "  neg = process_doc('txt_sentoken/neg/', vocab)\n",
        "  pos = process_doc('txt_sentoken/pos/', vocab)\n",
        "  doc = neg + pos \n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return doc, labels\n",
        "\n",
        "\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "doc, labels = load_clean_dataset(vocab)\n",
        "print(len(doc), len(labels))\n",
        "\n",
        "# prepare negative reviews \n",
        "# negative_lines = process_doc('txt_sentoken/neg/', vocab)\n",
        "# save_list(negative_lines, 'negative_reviews.txt')\n",
        "# prepare postive reviews \n",
        "# positive_lines = process_doc('txt_sentoken/pos/', vocab)\n",
        "# save_list(positive_lines, 'positive_reviews.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "2000 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii_eQBLV1aA2",
        "colab_type": "text"
      },
      "source": [
        "converting to vectors and spliting to train and test \n",
        "training samples = upto 899\n",
        "testing samples = 900 to 999"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP6zjxLtIYcY",
        "colab_type": "code",
        "outputId": "db66f6df-1bcd-43e9-c969-847f2f7ba64f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter \n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from string import punctuation\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "from pandas import DataFrame\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from matplotlib import pyplot\n",
        "\n",
        "def load_doc(filename):\n",
        "  #open the file as read only\n",
        "\n",
        "  file = open(filename, 'r')\n",
        "  # read the all the text \n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "def clean_doc(file):\n",
        "  tokens = file.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "  # remove punctuation from each word \n",
        "  tokens = [re_punc.sub('',w) for w in tokens]\n",
        "\n",
        "  # remove tokens that are not alphabets \n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "  # remove english stop words using NLTK\n",
        "  stopword = stopwords.words('english')\n",
        "  tokens = [w for w in tokens if not w in stopword]\n",
        "\n",
        "  # remove short tokens \n",
        "  tokens = [word for word in tokens if (len(word) > 1)]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "def doc_to_line(filename, vocab):\n",
        "  # load the document \n",
        "  doc = load_doc(filename)\n",
        "  # clean the document \n",
        "  tokens = clean_doc(doc)\n",
        "  # filter by vocab \n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  # in line \n",
        "  return ' '.join(tokens)\n",
        "\n",
        "def process_doc(directory, vocab, istrain):\n",
        "  lines = list()\n",
        "  for file in listdir(directory):\n",
        "    if istrain and file.startswith('cv9'):\n",
        "      continue\n",
        "    if not istrain and not file.startswith('cv9'):\n",
        "      continue\n",
        "    path = directory + '/' + file\n",
        "    line = doc_to_line(path, vocab)\n",
        "    lines.append(line)\n",
        "  return lines\n",
        "\n",
        "def sequential_model(x_train, x_test, y_train, y_test):\n",
        "  scores = list()\n",
        "  # for i in range(30):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(50, input_shape=(x_test.shape[1], ), activation='relu' ))\n",
        "  model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "  # compile model \n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  # fit model \n",
        "  model.fit(x_train, y_train, epochs = 50, verbose= 1)\n",
        "  loss, acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "  print('Test Accuracy: %f' % (acc*100))\n",
        "  scores.append(acc*100)\n",
        "  return scores\n",
        "\n",
        "# def prepare_data(train_data, test_data, mode):\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "x_train = tokenizer.texts_to_matrix(train_data, mode = mode)\n",
        "x_test = tokenizer.texts_to_matrix(test_data, mode=mode)\n",
        "  # return x_train, x_test\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "# load all training reviews \n",
        "positive_lines = process_doc('txt_sentoken/pos/', vocab, True)\n",
        "negative_lines = process_doc('txt_sentoken/neg/', vocab, True)\n",
        "train_data = positive_lines + negative_lines\n",
        "y_train = [1 for _ in range(900)] + [0 for _ in range(900)]\n",
        "\n",
        "\n",
        "# # create tokenizer \n",
        "# tokenizer = Tokenizer()\n",
        "\n",
        "# tokenizer.fit_on_texts(docs)\n",
        "\n",
        "# encode traning dataset \n",
        "# x_train = tokenizer.texts_to_matrix(docs, mode='freq')\n",
        "# print(x_train.shape)\n",
        "\n",
        "\n",
        "# load all testing reviews \n",
        "positive_lines = process_doc('txt_sentoken/pos/', vocab, False)\n",
        "negative_lines = process_doc('txt_sentoken/neg/', vocab, False)\n",
        "test_data = positive_lines + negative_lines\n",
        "# x_test = tokenizer.texts_to_matrix(docs, mode = 'freq')\n",
        "y_test = [1 for _ in range(100)] + [0 for _ in range(100)]\n",
        "# print(x_test.shape)\n",
        "\n",
        "modes = ['binary', 'tfidf', 'freq', 'count']\n",
        "results = DataFrame()\n",
        "for mode in modes:\n",
        "  # results = list()\n",
        "  x_train, x_test = prepare_data(train_data, test_data, mode)\n",
        "  results[mode] = sequential_model(x_train, x_test, y_train, y_test)\n",
        "  # results.append(result)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9feadb6a937a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# def prepare_data(train_data, test_data, mode):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xcLs0eZhUez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.pyplot import boxplot\n",
        "results.describe()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY0i2gdiOamH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.boxplot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-cbl_n1TzQa",
        "colab_type": "text"
      },
      "source": [
        "neural network model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-KoADIxWd8x",
        "colab_type": "text"
      },
      "source": [
        "We can see that we achieve the acc of 90.5% which is better than original paper accuracy which was 80 something using 10 fold cross validation. We can also see that model easily fits the training data with 50 epochs. \n",
        "\n",
        "lets try other word_scoring methods in text_to_matrix() \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMajFr7yiLhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter \n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from string import punctuation\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "from pandas import DataFrame\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "def load_doc(filename):\n",
        "  #open the file as read only\n",
        "\n",
        "  file = open(filename, 'r')\n",
        "  # read the all the text \n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "def clean_doc(file):\n",
        "  tokens = file.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "  # remove punctuation from each word \n",
        "  tokens = [re_punc.sub('',w) for w in tokens]\n",
        "\n",
        "  # remove tokens that are not alphabets \n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "  # remove english stop words using NLTK\n",
        "  stopword = stopwords.words('english')\n",
        "  tokens = [w for w in tokens if not w in stopword]\n",
        "\n",
        "  # remove short tokens \n",
        "  tokens = [word for word in tokens if (len(word) > 1)]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "def doc_to_line(filename, vocab):\n",
        "  # load the document \n",
        "  doc = load_doc(filename)\n",
        "  # clean the document \n",
        "  tokens = clean_doc(doc)\n",
        "  # filter by vocab \n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  # in line \n",
        "  return ' '.join(tokens)\n",
        "\n",
        "def process_doc(directory, vocab):\n",
        "  lines = list()\n",
        "  for file in listdir(directory):\n",
        "    # if istrain and file.startswith('cv9'):\n",
        "    #   continue\n",
        "    # if not istrain and not file.startswith('cv9'):\n",
        "    #   continue\n",
        "    path = directory + '/' + file\n",
        "    line = doc_to_line(path, vocab)\n",
        "    lines.append(line)\n",
        "  return lines\n",
        "\n",
        "# def save_list(lines, filename):\n",
        "#   data = '\\n'.join(lines)\n",
        "#   file = open(filename, 'w')\n",
        "#   file.write(data)\n",
        "#   file.close()\n",
        "\n",
        "\n",
        "def load_clean_dataset(vocab):\n",
        "  neg = process_doc('txt_sentoken/neg/', vocab)\n",
        "  pos = process_doc('txt_sentoken/pos/', vocab)\n",
        "  doc = neg + pos \n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return doc, labels\n",
        "\n",
        "def sequential_model():\n",
        "  scores = list()\n",
        "  # for i in range(30):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(50, input_shape=(x_test.shape[1], ), activation='relu' ))\n",
        "  model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "  # compile model \n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "train_doc, y_train = load_clean_dataset(vocab)\n",
        "test_doc, y_test = load_clean_dataset(vocab)\n",
        "\n",
        "# def prepare_data(train_data, test_data, mode):\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_doc)\n",
        "x_train = tokenizer.texts_to_matrix(train_doc, mode = 'binary')\n",
        "x_test = tokenizer.texts_to_matrix(test_doc, mode='binary')\n",
        "  # return x_train, x_test\n",
        "\n",
        "# load all training reviews \n",
        "# positive_lines = process_doc('txt_sentoken/pos/', vocab, True)\n",
        "# negative_lines = process_doc('txt_sentoken/neg/', vocab, True)\n",
        "# train_data = positive_lines + negative_lines\n",
        "# y_train = [1 for _ in range(positive_lines)] + [0 for _ in range(negative_lines)]\n",
        "\n",
        "\n",
        "# # create tokenizer \n",
        "# tokenizer = Tokenizer()\n",
        "\n",
        "# tokenizer.fit_on_texts(docs)\n",
        "\n",
        "# encode traning dataset \n",
        "# x_train = tokenizer.texts_to_matrix(docs, mode='freq')\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "# load all testing reviews \n",
        "# positive_lines = process_doc('txt_sentoken/pos/', vocab)\n",
        "# negative_lines = process_doc('txt_sentoken/neg/', vocab)\n",
        "# test_data = positive_lines + negative_lines\n",
        "# x_test = tokenizer.texts_to_matrix(docs, mode = 'freq')\n",
        "# y_test = [1 for _ in range(100)] + [0 for _ in range(100)]\n",
        "# print(x_test.shape)\n",
        "\n",
        "# modes = ['binary', 'tfidf', 'freq', 'count']\n",
        "# results = DataFrame()\n",
        "# for mode in modes:\n",
        "  # results = list()\n",
        "# x_train, x_test = prepare_data(train_data, test_data, mode)\n",
        "  # results[mode] = sequential_model()\n",
        "  # results.append(result)\n",
        "# print(results)\n",
        "\n",
        "model = sequential_model()\n",
        "# fit model \n",
        "model.fit(x_train, y_train, epochs = 50, verbose= 1)\n",
        "# loss, acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "# print('Test Accuracy: %f' % (acc*100))\n",
        "# scores.append(acc*100)\n",
        "# return scores\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3etog1WXfYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = sequential_model()\n",
        "\n",
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        "  tokens = clean_doc(review)\n",
        "\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  line = ' '.join(tokens)\n",
        "\n",
        "  # encode \n",
        "  encoded = tokenizer.texts_to_matrix([line], mode = 'binary')\n",
        "\n",
        "  # predict sentiment \n",
        "  prediction = model.predict(encoded, verbose =1)\n",
        "\n",
        "  # retrieve predicted percentage and label \n",
        "  percent_pos = prediction[0,0]\n",
        "  if round(percent_pos) == 0:\n",
        "    return (1 - percent_pos), 'Negative' \n",
        "  return percent_pos, 'Positive'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwtVpI4KXEWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test positive text \n",
        "text = 'Best movie ever! It was great, I recommend it.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: [%s]\\nPercent: [%.3f]' % (text, sentiment, percent*100))\n",
        "\n",
        "# test negative text \n",
        "text = 'This is a bad movie.' \n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model) \n",
        "print('Review: [%s]\\nSentiment: [%s]\\nPercent: [%.3f]' % (text, sentiment, percent*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN3R2hnfgRCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}